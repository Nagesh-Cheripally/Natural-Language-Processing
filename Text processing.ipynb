{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">Text Preprocessing with NLTK</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import nltk package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the NLTK copora and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run cell to download the libraries if you don't have\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the libraries we will use for text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize # spliting string into substrings\n",
    "from nltk.corpus import wordnet # for synonyms\n",
    "from nltk.corpus import stopwords # for removing stop words\n",
    "from nltk.stem import PorterStemmer # for stemming a word\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tree import Tree\n",
    "import string # to remove punctuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with the given phrase ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, \\nand artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to \\nprocess and analyze large amounts of natural language data. The history of natural language processing (NLP) generally started \\nin the 1950s, although work can be found from earlier periods. In 1950\\'s, Alan Turing published an article titled \\n\"Computing Machinery and Intelligence \"which proposed what is now called the Turing test as a criterion of @intelligence goes'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"\"\"Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, \n",
    "and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to \n",
    "process and analyze large amounts of natural language data. The history of natural language processing (NLP) generally started \n",
    "in the 1950s, although work can be found from earlier periods. In 1950's, Alan Turing published an article titled \n",
    "\"Computing Machinery and Intelligence \"which proposed what is now called the Turing test as a criterion of @intelligence goes\"\"\"\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div align=\"center\">Counting the number of characters in a text</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "628"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural la'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Returns all characters from index 0 to 10\n",
    "text[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selects a character at index 10-1 which is index 9\n",
    "text[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div align=\"center\">Removing Punctuation</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print available punctuation recognized by Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural language processing NLP is a subfield of linguistics computer science information engineering \\nand artificial intelligence concerned with the interactions between computers and human natural languages in particular how to program computers to \\nprocess and analyze large amounts of natural language data The history of natural language processing NLP generally started \\nin the 1950s although work can be found from earlier periods In 1950s Alan Turing published an article titled \\nComputing Machinery and Intelligence which proposed what is now called the Turing test as a criterion of intelligence goes'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join([t for t in text if t not in string.punctuation])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting text to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural language processing (nlp) is a subfield of linguistics, computer science, information engineering, \n",
      "and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to \n",
      "process and analyze large amounts of natural language data. the history of natural language processing (nlp) generally started \n",
      "in the 1950s, although work can be found from earlier periods. in 1950's, alan turing published an article titled \n",
      "\"computing machinery and intelligence \"which proposed what is now called the turing test as a criterion of @intelligence goes\n"
     ]
    }
   ],
   "source": [
    "text=\"\".join([t.lower() for t in text ])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div align=\"center\">Tokenization</div>\n",
    "Tokenization is the prcess of spliting a text into constituent substring. We can split a text by sentence or words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize text to words.\n",
    "The function contains the following arguments word_tokenize(text, language='english', preserve_line=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', '(', 'nlp', ')', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'information', 'engineering', ',', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.', 'the', 'history', 'of', 'natural', 'language', 'processing', '(', 'nlp', ')', 'generally', 'started', 'in', 'the', '1950s', ',', 'although', 'work', 'can', 'be', 'found', 'from', 'earlier', 'periods', '.', 'in', '1950', \"'s\", ',', 'alan', 'turing', 'published', 'an', 'article', 'titled', \"''\", 'computing', 'machinery', 'and', 'intelligence', '``', 'which', 'proposed', 'what', 'is', 'now', 'called', 'the', 'turing', 'test', 'as', 'a', 'criterion', 'of', '@', 'intelligence', 'goes']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text=word_tokenize(text,language='english', preserve_line=False)\n",
    "print(tokenized_text) # select only the first 20 words from the list using [0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize text to sentences.\n",
    "The function contains the following arguments sent_tokenize(text, language='english', preserve_line=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural language processing (nlp) is a subfield of linguistics, computer science, information engineering, \\nand artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to \\nprocess and analyze large amounts of natural language data.',\n",
       " 'the history of natural language processing (nlp) generally started \\nin the 1950s, although work can be found from earlier periods.',\n",
       " 'in 1950\\'s, alan turing published an article titled \\n\"computing machinery and intelligence \"which proposed what is now called the turing test as a criterion of @intelligence goes']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text,language='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenie words in each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['natural', 'language', 'processing', '(', 'nlp'],\n",
       " ['the', 'history', 'of', 'natural', 'language'],\n",
       " ['in', '1950', \"'s\", ',', 'alan']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word_tokenize(t)[0:5] for t in sent_tokenize(text,language='english')] #  display only the first 5 words in each sentence using [0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div align=\"center\">Removing stop words</div>\n",
    "Stop words are a set of commonly used words in any language. For example, in English, “the”, “is” and “and”, would easily qualify as stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show stop words from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"english\")[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stop words from the entire text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', '(', 'nlp', ')', 'subfield', 'linguistics', ',', 'computer', 'science', ',', 'information', 'engineering', ',', 'artificial', 'intelligence', 'concerned', 'interactions', 'computers', 'human', '(', 'natural', ')', 'languages', ',', 'particular', 'program', 'computers', 'process', 'analyze', 'large', 'amounts', 'natural', 'language', 'data', '.', 'history', 'natural', 'language', 'processing', '(', 'nlp', ')', 'generally', 'started', '1950s', ',', 'although', 'work', 'found', 'earlier', 'periods', '.', '1950', \"'s\", ',', 'alan', 'turing', 'published', 'article', 'titled', \"''\", 'computing', 'machinery', 'intelligence', '``', 'proposed', 'called', 'turing', 'test', 'criterion', '@', 'intelligence', 'goes']\n"
     ]
    }
   ],
   "source": [
    "text=[w for w in tokenized_text if not w in stopwords.words('english')]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div align=\"center\">Text Normalization</div>\n",
    "##### 1. Stemming\n",
    "Stemming is the process of reducing a word into its root/base e.g sleeping to sleep, eating to eat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming a single word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'natur'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PorterStemmer().stem('Natural')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goe\n",
      "go\n",
      "go\n"
     ]
    }
   ],
   "source": [
    "print(PorterStemmer().stem('goes'))\n",
    "print(PorterStemmer().stem('going'))\n",
    "print(PorterStemmer().stem('go'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur', 'languag', 'process', '(', 'nlp', ')', 'subfield', 'linguist', ',', 'comput', 'scienc', ',', 'inform', 'engin', ',', 'artifici', 'intellig', 'concern', 'interact', 'comput', 'human', '(', 'natur', ')', 'languag', ',', 'particular', 'program', 'comput', 'process', 'analyz', 'larg', 'amount', 'natur', 'languag', 'data', '.', 'histori', 'natur', 'languag', 'process', '(', 'nlp', ')', 'gener', 'start', '1950', ',', 'although', 'work', 'found', 'earlier', 'period', '.', '1950', \"'s\", ',', 'alan', 'ture', 'publish', 'articl', 'titl', \"''\", 'comput', 'machineri', 'intellig', '``', 'propos', 'call', 'ture', 'test', 'criterion', '@', 'intellig', 'goe']\n"
     ]
    }
   ],
   "source": [
    "stemmed_text = [PorterStemmer().stem(word) for word in text]\n",
    "print(stemmed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Lemmatization\n",
    "Lemmatization is the process of reducing a word into its base/root but taking into consideration the morphological analysis of the word. Unlike stemming which cuts off the ending or starting characters of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatize a single word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordNetLemmatizer().lemmatize('Natural')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "go\n",
      "going\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize('goes'))\n",
    "print(WordNetLemmatizer().lemmatize('go'))\n",
    "print(WordNetLemmatizer().lemmatize('going'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatize the entire text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', '(', 'nlp', ')', 'subfield', 'linguistics', ',', 'computer', 'science', ',', 'information', 'engineering', ',', 'artificial', 'intelligence', 'concerned', 'interaction', 'computer', 'human', '(', 'natural', ')', 'language', ',', 'particular', 'program', 'computer', 'process', 'analyze', 'large', 'amount', 'natural', 'language', 'data', '.', 'history', 'natural', 'language', 'processing', '(', 'nlp', ')', 'generally', 'started', '1950s', ',', 'although', 'work', 'found', 'earlier', 'period', '.', '1950', \"'s\", ',', 'alan', 'turing', 'published', 'article', 'titled', \"''\", 'computing', 'machinery', 'intelligence', '``', 'proposed', 'called', 'turing', 'test', 'criterion', '@', 'intelligence', 'go']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_text=[WordNetLemmatizer().lemmatize(t) for t in text]\n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div align=\"center\">Synonyms and Antonyms</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synonyms<br>\n",
    "Synonym is a word or phrase that means exactly or nearly the same as another word or phrase in the same language, for example shut is a synonym of close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('state.n.04'),\n",
       " Synset('country.n.02'),\n",
       " Synset('nation.n.02'),\n",
       " Synset('country.n.04'),\n",
       " Synset('area.n.01')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn=wordnet.synsets('country')\n",
    "syn # Returns a list of synonyms for the above word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the first synonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'state.n.04'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn[0].name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "definistion of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a politically organized body of people under a single government'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn[0].definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of how the sysnonym word has been used in sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the state has elected a new president',\n",
       " 'African nations',\n",
       " \"students who had come to the nation's capitol\",\n",
       " \"the country's largest manufacturer\",\n",
       " 'an industrialized land']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn[0].examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the synonym word using lemmas function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'state'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn[0].lemmas()[0].name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antonyms<br/>\n",
    "Antonyms is a word opposite in meaning to another (e.g. bad and good ).\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms for the word country\n",
      " ['happy', 'felicitous', 'happy', 'glad', 'happy', 'happy', 'well-chosen'] \n",
      "Antonyms for the the synonyms\n",
      " ['unhappy']\n"
     ]
    }
   ],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "  \n",
    "for syn in wordnet.synsets(\"happy\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "            \n",
    "print('Synonyms for the word country\\n',synonyms,'\\nAntonyms for the the synonyms\\n',antonyms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div align=\"center\">Part of Speech Tagging (POS Tagging)</div>\n",
    "<br >POS Tagging is the process of assigining the gramatical class (tag) to words in a text.\n",
    "Technique for POS Tagging;\n",
    "1. Rule-based tagging. Used for ambiguous words.\n",
    "2. Stochastic tagging. This is a probabilistic approach where the tag is computed from the probability of occurence of the word category \n",
    "3. Hidden MArkov Model (HMM). This technique asigns tags to words by assessing the hiddent state of tags in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('natural', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('processing', 'NN'),\n",
       " ('(', '('),\n",
       " ('nlp', 'JJ'),\n",
       " (')', ')'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('subfield', 'NN'),\n",
       " ('of', 'IN')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos=pos_tag(tokenized_text)\n",
    "pos[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div align=\"center\">Named Entity Recognition (NER)</div>\n",
    "<br>This is the process of classifying text that either belongs to person, location, organization, quantities e.t.c. There are various technique for the NER such as chunking and Stanford NER library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER using chunking technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_text=\"Donald Trump is the president of the USA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting NER Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Donald/NNP)\n",
      "  (ORGANIZATION Trump/NNP)\n",
      "  is/VBZ\n",
      "  the/DT\n",
      "  president/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION USA/NNP))\n"
     ]
    }
   ],
   "source": [
    "ner_text=word_tokenize(ner_text)\n",
    "ner_text=pos_tag(ner_text)\n",
    "named_entities = ne_chunk(ner_text)\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through the NER Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('S', [Tree('PERSON', [('Donald', 'NNP')]), Tree('ORGANIZATION', [('Trump', 'NNP')]), ('is', 'VBZ'), ('the', 'DT'), ('president', 'NN'), ('of', 'IN'), ('the', 'DT'), Tree('ORGANIZATION', [('USA', 'NNP')])]),\n",
       " Tree('PERSON', [('Donald', 'NNP')]),\n",
       " Tree('ORGANIZATION', [('Trump', 'NNP')]),\n",
       " Tree('ORGANIZATION', [('USA', 'NNP')])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities=[]\n",
    "for ne in named_entities.subtrees():\n",
    "#     if ne.label()=='ORGANIZATION': // use this to return only entities of specific class e.g ORGANIZATION, PERSON, LOCATION e.t.c\n",
    "        entities.append(ne)\n",
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a nice list of NER for easy manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Donald', 'PERSON'), ('Trump', 'ORGANIZATION'), ('USA', 'ORGANIZATION')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse named entities from tree\n",
    "ne = []\n",
    "for subtree in named_entities:\n",
    "    if type(subtree) == Tree:\n",
    "        ne_label = subtree.label()\n",
    "        ne_string = \" \".join([token for token, pos in subtree.leaves()])\n",
    "        ne.append((ne_string, ne_label))\n",
    "            \n",
    "ne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div align='center'>Vectorization</div>\n",
    "This is the process of encoding text data into integers (vectors) where a compter model can understand and process.\n",
    "Techniques for Text Vectorization Include;\n",
    "1. Bag of Words - Count Vectorizations\n",
    "2. Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "3. Hashing Vectorization\n",
    "4. Word Embedding - Word2Vec\n",
    "5. Sentence Embedding - Sent2Vec\n",
    "6. Document Embedding - Doc2Vec\n",
    "7. Character Embedding - Char2Vec\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div align='center'>Document Term Matrix with Count Vectorizations</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document term matrix is is a mathematical matrix that describes the frequency of terms that occur in a collection of document. Each row represents  document and columns represents terms.<br><br>\n",
    "For the Count Vectorization the results is a sparse matrix with the count frequency of occurence of each word/term in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=['My name is Sammy.',\n",
    "       'Donald is the President of USA.',\n",
    "       'London is a city in England.'\n",
    "       'It\\'s winter in London.']\n",
    "\n",
    "vectorizer=CountVectorizer()\n",
    "x=vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['city', 'donald', 'england', 'in', 'is', 'it', 'london', 'my', 'name', 'of', 'president', 'sammy', 'the', 'usa', 'winter']\n"
     ]
    }
   ],
   "source": [
    "# Get unique words in the corpus\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 0 1 1 0 0 1 0 0 0]\n",
      " [0 1 0 0 1 0 0 0 0 1 1 0 1 1 0]\n",
      " [1 0 1 2 1 1 2 0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Get sparse matrix of the terms/words.\n",
    "print(x.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>donald</th>\n",
       "      <th>england</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>london</th>\n",
       "      <th>my</th>\n",
       "      <th>name</th>\n",
       "      <th>of</th>\n",
       "      <th>president</th>\n",
       "      <th>sammy</th>\n",
       "      <th>the</th>\n",
       "      <th>usa</th>\n",
       "      <th>winter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   city  donald  england  in  is  it  london  my  name  of  president  sammy  \\\n",
       "0     0       0        0   0   1   0       0   1     1   0          0      1   \n",
       "1     0       1        0   0   1   0       0   0     0   1          1      0   \n",
       "2     1       0        1   2   1   1       2   0     0   0          0      0   \n",
       "\n",
       "   the  usa  winter  \n",
       "0    0    0       0  \n",
       "1    1    1       0  \n",
       "2    0    0       1  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving results of the document term matrix to a pandas dataframe\n",
    "count_vectorizer_df=pd.DataFrame(x.toarray(),columns=vectorizer.get_feature_names())\n",
    "count_vectorizer_df.head()\n",
    "\n",
    "# The word/term city occures once in the third document while the words/terms in and london occures twice in the third document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['city in', 'donald is', 'england it', 'in england', 'in london', 'is city', 'is sammy', 'is the', 'it winter', 'london is', 'my name', 'name is', 'of usa', 'president of', 'the president', 'winter in']\n",
      "[[0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 0]\n",
      " [1 0 1 1 1 1 0 0 1 1 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Fit the vectorizer to return n_gram words using the bi_gram technique\n",
    "cv2=CountVectorizer(ngram_range=(2,2))\n",
    "x2=cv2.fit_transform(corpus)\n",
    "print(cv2.get_feature_names())\n",
    "print(x2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div align='center'>Document Term Matrix with Term Frequency Inverse Document Frequency (tf-idf)</div><br>\n",
    "The term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x15 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 17 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus=['My name is Sammy.',\n",
    "       'Donald is the President of USA.',\n",
    "       'London is a city in England.'\n",
    "       'It\\'s winter in London.']\n",
    "\n",
    "tf_idf_vectorizer=TfidfVectorizer() # Initialize tf-idf\n",
    "x=tf_idf_vectorizer.fit_transform(corpus)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['city', 'donald', 'england', 'in', 'is', 'it', 'london', 'my', 'name', 'of', 'president', 'sammy', 'the', 'usa', 'winter']\n"
     ]
    }
   ],
   "source": [
    "# Get unique words in the corpus\n",
    "print(tf_idf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.32274454 0.\n",
      "  0.         0.54645401 0.54645401 0.         0.         0.54645401\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.43238509 0.         0.         0.2553736  0.\n",
      "  0.         0.         0.         0.43238509 0.43238509 0.\n",
      "  0.43238509 0.43238509 0.        ]\n",
      " [0.28456871 0.         0.28456871 0.56913741 0.16807086 0.28456871\n",
      "  0.56913741 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.28456871]]\n"
     ]
    }
   ],
   "source": [
    "# Get sparse matrix of the terms/words. tf-idf returns the weithed values for each term in the corpus unlike countvectorizer that returns the count of terms in a corpus\n",
    "print(x.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>donald</th>\n",
       "      <th>england</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>london</th>\n",
       "      <th>my</th>\n",
       "      <th>name</th>\n",
       "      <th>of</th>\n",
       "      <th>president</th>\n",
       "      <th>sammy</th>\n",
       "      <th>the</th>\n",
       "      <th>usa</th>\n",
       "      <th>winter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.322745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.432385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.255374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.432385</td>\n",
       "      <td>0.432385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.432385</td>\n",
       "      <td>0.432385</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.284569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284569</td>\n",
       "      <td>0.569137</td>\n",
       "      <td>0.168071</td>\n",
       "      <td>0.284569</td>\n",
       "      <td>0.569137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       city    donald   england        in        is        it    london  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.322745  0.000000  0.000000   \n",
       "1  0.000000  0.432385  0.000000  0.000000  0.255374  0.000000  0.000000   \n",
       "2  0.284569  0.000000  0.284569  0.569137  0.168071  0.284569  0.569137   \n",
       "\n",
       "         my      name        of  president     sammy       the       usa  \\\n",
       "0  0.546454  0.546454  0.000000   0.000000  0.546454  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.432385   0.432385  0.000000  0.432385  0.432385   \n",
       "2  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "     winter  \n",
       "0  0.000000  \n",
       "1  0.000000  \n",
       "2  0.284569  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a dataframe of tf-idf document term matrix\n",
    "tf_idf_df=pd.DataFrame(x.toarray(),columns=tf_idf_vectorizer.get_feature_names())\n",
    "tf_idf_df.head()\n",
    "\n",
    "# Each word/term is assigned a threshold. The words that occure frequently have lower weight making them less important unlike rare words that have high weights making them ore important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div align='center'>Document Term Matrix with Hashing Vecorization</div><br>\n",
    "This approach converts tokens into sparse matrix by hashing them. It requires low memory but there can be a collision where several tokens are hashed to the same space. This approach is useful in streaming and parallel pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x1048576 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 17 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus=['My name is Sammy.',\n",
    "       'Donald is the President of USA.',\n",
    "       'London is a city in England.'\n",
    "       'It\\'s winter in London.']\n",
    "\n",
    "hashing_vectorizer=HashingVectorizer() \n",
    "x=hashing_vectorizer.fit_transform(corpus)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Get sparse matrix\n",
    "print(x.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1048566</th>\n",
       "      <th>1048567</th>\n",
       "      <th>1048568</th>\n",
       "      <th>1048569</th>\n",
       "      <th>1048570</th>\n",
       "      <th>1048571</th>\n",
       "      <th>1048572</th>\n",
       "      <th>1048573</th>\n",
       "      <th>1048574</th>\n",
       "      <th>1048575</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 1048576 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0        1        2        3        4        5        6        7        \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   8        9         ...     1048566  1048567  1048568  1048569  1048570  \\\n",
       "0      0.0      0.0   ...         0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0   ...         0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0   ...         0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   1048571  1048572  1048573  1048574  1048575  \n",
       "0      0.0      0.0      0.0      0.0      0.0  \n",
       "1      0.0      0.0      0.0      0.0      0.0  \n",
       "2      0.0      0.0      0.0      0.0      0.0  \n",
       "\n",
       "[3 rows x 1048576 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a dataframe of sparse document term matrix\n",
    "tf_idf_df=pd.DataFrame(x.toarray())\n",
    "tf_idf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
