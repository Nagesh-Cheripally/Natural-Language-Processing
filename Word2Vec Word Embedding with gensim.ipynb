{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='center'><h1>Word Embedding With gensim</h1></div>\n",
    "\n",
    "Word embedding also reffered to as distributed semantic model or semantic vector space or vector space model is a language modeling and feature learning technique that clusters similar words together. The most commond word embedding vectors are word2vec (Google), Glove (Stanford) and fastest (Facebook). Word embedding is applied in text classification, document clustering,dimensional reduction,topic modeling, feature generation among other NLP tasks such as sentiment analysis.<p>\n",
    "Unlike the Latent Semantic Analysis technique that ignores words order and context the word embedding technique considers the order of words and context in the sentence.</p>\n",
    "<p>Word2Vec employs the Continous Bag of Word model (CBOW) and Skip-gram model</p>\n",
    "<p>gensim is a python library for topic modeling. It's used to analyse text-documents for semantic relationship and other NLP tasks.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('datasets/Sentiment Analysis on Movie Reviews/train.tsv',sep='\\t')\n",
    "test=pd.read_csv('datasets/Sentiment Analysis on Movie Reviews/test.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a tokenized text below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [['first', 'sentence'], ['second', 'sentence'],['The','quick','brown','fox','jumped','over','the','lazy','dog']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['first', 'sentence'],\n",
       " ['second', 'sentence'],\n",
       " ['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating our word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x214982ffb70>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=gensim.models.Word2Vec(sentence, min_count=1)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve trained tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'first': <gensim.models.keyedvectors.Vocab object at 0x00000214982FF668>, 'sentence': <gensim.models.keyedvectors.Vocab object at 0x00000214982FF6A0>, 'second': <gensim.models.keyedvectors.Vocab object at 0x00000214982FF6D8>, 'The': <gensim.models.keyedvectors.Vocab object at 0x00000214982FF710>, 'quick': <gensim.models.keyedvectors.Vocab object at 0x00000214982FF7F0>, 'brown': <gensim.models.keyedvectors.Vocab object at 0x00000214982FF908>, 'fox': <gensim.models.keyedvectors.Vocab object at 0x00000214982FFA20>, 'jumped': <gensim.models.keyedvectors.Vocab object at 0x00000214982FF7B8>, 'over': <gensim.models.keyedvectors.Vocab object at 0x00000214982FFD30>, 'the': <gensim.models.keyedvectors.Vocab object at 0x00000214982FFE10>, 'lazy': <gensim.models.keyedvectors.Vocab object at 0x00000214982FFE80>, 'dog': <gensim.models.keyedvectors.Vocab object at 0x00000214982FFEB8>}\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve specific token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.2133033e-03, -4.1468404e-03,  3.5501192e-03,  1.9773499e-03,\n",
       "       -3.8439210e-03, -2.3370753e-03, -2.5862933e-03,  4.7785128e-03,\n",
       "       -2.3628618e-03, -3.9900574e-03, -4.3501174e-03,  4.3709385e-03,\n",
       "        1.5694880e-03,  4.9045408e-04, -4.8499834e-03,  4.4760057e-03,\n",
       "        3.4979009e-03,  1.5725266e-03, -2.4954190e-03,  2.1538648e-03,\n",
       "       -7.4231252e-04, -4.8448308e-03, -2.1602346e-03,  1.0299633e-03,\n",
       "        1.9122426e-03, -2.1443358e-03, -8.6653850e-04, -6.2897825e-04,\n",
       "       -6.6145009e-04, -7.5320924e-07, -2.9841023e-03, -3.9988770e-03,\n",
       "        1.1172598e-03, -3.9438014e-03,  1.2881323e-03, -1.5997713e-03,\n",
       "        2.4644195e-03,  3.9324677e-03,  1.9670150e-04,  8.6464963e-05,\n",
       "       -3.0795226e-03,  2.9440979e-03, -4.5456258e-03,  4.0142573e-03,\n",
       "        1.1610993e-03, -2.7284995e-03, -3.7780118e-03,  3.3455668e-03,\n",
       "        4.1449093e-03, -2.6031544e-03,  6.0553110e-04, -9.6098846e-04,\n",
       "        3.3479708e-03, -4.9109398e-03, -3.7751268e-03, -1.4853446e-03,\n",
       "        3.7259543e-03, -3.1156014e-03, -1.7316173e-03,  4.8403363e-03,\n",
       "       -2.3044217e-03,  5.6235283e-04,  2.4243300e-03, -3.4330962e-03,\n",
       "       -3.0100141e-03, -4.8197396e-03, -1.3465775e-03,  2.7568578e-03,\n",
       "       -1.9785275e-03, -4.3627131e-03,  9.7197382e-04, -1.1885876e-03,\n",
       "       -2.2182900e-03,  1.1408463e-03,  4.9726139e-03, -1.8368730e-03,\n",
       "        4.3447572e-03,  3.9645219e-03, -3.6909557e-03,  8.6469826e-04,\n",
       "       -4.8588058e-03,  9.1906870e-04, -4.6371599e-03,  1.3621761e-03,\n",
       "        4.7467073e-04,  4.0476801e-04,  9.0191286e-04, -2.7284634e-04,\n",
       "        3.2845761e-03,  8.8234682e-04,  5.2020606e-04, -1.7838245e-03,\n",
       "       -3.9002432e-03,  4.2284154e-03, -1.4095114e-03, -3.6579054e-03,\n",
       "       -2.7110074e-03,  4.3201344e-03, -4.6378975e-03, -2.4274094e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['brown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.4020096e-03, -4.2423438e-03,  3.7243478e-03, -3.7726329e-03,\n",
       "        4.0324302e-03,  3.0911160e-03,  7.0586108e-04, -1.1325873e-03,\n",
       "        4.8682434e-03,  1.7595576e-03, -1.5104869e-03, -1.4333397e-03,\n",
       "        1.6859388e-03, -2.8028684e-03,  1.4997101e-03, -1.7186148e-04,\n",
       "       -1.2604368e-03,  1.0603896e-03, -1.1595790e-03, -4.8192181e-03,\n",
       "       -1.7920536e-03, -2.7532284e-03, -3.1439932e-03, -3.9328909e-03,\n",
       "       -5.1050307e-04,  3.9696470e-03,  3.1739920e-03, -1.5088958e-03,\n",
       "       -3.1638080e-03, -2.3056897e-03,  3.6154471e-03, -4.1556987e-03,\n",
       "        4.2755823e-04, -8.8977389e-04, -3.5432708e-03, -4.7446574e-05,\n",
       "        3.9908825e-04,  3.7676224e-03, -3.3464681e-03,  2.8239053e-03,\n",
       "       -4.9581355e-03,  4.6536541e-03,  9.1226968e-05, -2.4793202e-03,\n",
       "        3.8798878e-03,  4.5912960e-03,  1.2949150e-03, -9.9112069e-05,\n",
       "       -2.0052127e-03, -3.4109557e-03, -4.1654422e-03,  1.0489922e-03,\n",
       "       -3.2214606e-03, -2.7957771e-03,  1.7985987e-03, -2.7461911e-03,\n",
       "        9.8920998e-04,  3.0019418e-03, -4.6293372e-03, -2.9089328e-04,\n",
       "       -3.8963140e-03, -4.7605648e-03, -3.9496049e-03, -1.5594627e-03,\n",
       "        2.1947690e-03,  4.0800283e-03, -1.8042491e-03, -4.0398738e-03,\n",
       "       -3.1129599e-03, -3.9744498e-03, -4.8743505e-03, -2.5295215e-03,\n",
       "       -1.2714247e-03, -2.3908648e-03, -2.3670522e-03,  8.3362240e-05,\n",
       "        2.6825750e-03, -1.0460267e-03, -4.0803435e-03,  1.5618938e-03,\n",
       "        3.8899912e-03,  3.6626068e-04,  7.0110470e-04, -3.7404853e-03,\n",
       "       -1.0882312e-03,  4.7207121e-03,  4.8431284e-03, -4.1086990e-03,\n",
       "        4.5353584e-03, -8.3605095e-04, -3.9801686e-03, -2.1307401e-03,\n",
       "        1.1478983e-03, -2.2128283e-04, -2.1541901e-03, -2.9312840e-03,\n",
       "        4.0913248e-03,  3.3597418e-03, -2.3511141e-03,  1.2837694e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['fox']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Out of Vocabulary word problem (OOV)<h5 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a major weakness of the word embedding where words that are not represented in the training corpus are not mapped to any vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the word 'India' which is not in the training corpus we get an error since the model doesn't recognize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'India' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-f011f922562c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'India'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1459\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m                 )\n\u001b[1;32m-> 1461\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1463\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \"\"\"\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Method will be removed in 4.0.0, use self.wv.__contains__() instead\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, entities)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[1;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'India' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model['India']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Out of Vocabulary word problem is also case sensitive i.e the word fox and Fox are two different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.4020096e-03, -4.2423438e-03,  3.7243478e-03, -3.7726329e-03,\n",
       "        4.0324302e-03,  3.0911160e-03,  7.0586108e-04, -1.1325873e-03,\n",
       "        4.8682434e-03,  1.7595576e-03, -1.5104869e-03, -1.4333397e-03,\n",
       "        1.6859388e-03, -2.8028684e-03,  1.4997101e-03, -1.7186148e-04,\n",
       "       -1.2604368e-03,  1.0603896e-03, -1.1595790e-03, -4.8192181e-03,\n",
       "       -1.7920536e-03, -2.7532284e-03, -3.1439932e-03, -3.9328909e-03,\n",
       "       -5.1050307e-04,  3.9696470e-03,  3.1739920e-03, -1.5088958e-03,\n",
       "       -3.1638080e-03, -2.3056897e-03,  3.6154471e-03, -4.1556987e-03,\n",
       "        4.2755823e-04, -8.8977389e-04, -3.5432708e-03, -4.7446574e-05,\n",
       "        3.9908825e-04,  3.7676224e-03, -3.3464681e-03,  2.8239053e-03,\n",
       "       -4.9581355e-03,  4.6536541e-03,  9.1226968e-05, -2.4793202e-03,\n",
       "        3.8798878e-03,  4.5912960e-03,  1.2949150e-03, -9.9112069e-05,\n",
       "       -2.0052127e-03, -3.4109557e-03, -4.1654422e-03,  1.0489922e-03,\n",
       "       -3.2214606e-03, -2.7957771e-03,  1.7985987e-03, -2.7461911e-03,\n",
       "        9.8920998e-04,  3.0019418e-03, -4.6293372e-03, -2.9089328e-04,\n",
       "       -3.8963140e-03, -4.7605648e-03, -3.9496049e-03, -1.5594627e-03,\n",
       "        2.1947690e-03,  4.0800283e-03, -1.8042491e-03, -4.0398738e-03,\n",
       "       -3.1129599e-03, -3.9744498e-03, -4.8743505e-03, -2.5295215e-03,\n",
       "       -1.2714247e-03, -2.3908648e-03, -2.3670522e-03,  8.3362240e-05,\n",
       "        2.6825750e-03, -1.0460267e-03, -4.0803435e-03,  1.5618938e-03,\n",
       "        3.8899912e-03,  3.6626068e-04,  7.0110470e-04, -3.7404853e-03,\n",
       "       -1.0882312e-03,  4.7207121e-03,  4.8431284e-03, -4.1086990e-03,\n",
       "        4.5353584e-03, -8.3605095e-04, -3.9801686e-03, -2.1307401e-03,\n",
       "        1.1478983e-03, -2.2128283e-04, -2.1541901e-03, -2.9312840e-03,\n",
       "        4.0913248e-03,  3.3597418e-03, -2.3511141e-03,  1.2837694e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['fox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'Fox' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-be20884dcae8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Fox'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1459\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m                 )\n\u001b[1;32m-> 1461\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1463\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \"\"\"\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Method will be removed in 4.0.0, use self.wv.__contains__() instead\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, entities)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[1;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'Fox' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model['Fox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vecors(text):\n",
    "    doc_vectors=[word for word in list(text) if word in model.wv.vocab]\n",
    "    return np.mean(model[doc_vectors], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.8076565e-03, -4.1945921e-03,  3.6372335e-03, -8.9764153e-04,\n",
       "        9.4254618e-05,  3.7702033e-04, -9.4021612e-04,  1.8229627e-03,\n",
       "        1.2526908e-03, -1.1152499e-03, -2.9303022e-03,  1.4687994e-03,\n",
       "        1.6277134e-03, -1.1562072e-03, -1.6751366e-03,  2.1520720e-03,\n",
       "        1.1187320e-03,  1.3164581e-03, -1.8274990e-03, -1.3326766e-03,\n",
       "       -1.2671831e-03, -3.7990296e-03, -2.6521138e-03, -1.4514639e-03,\n",
       "        7.0086977e-04,  9.1265561e-04,  1.1537268e-03, -1.0689370e-03,\n",
       "       -1.9126290e-03, -1.1532215e-03,  3.1567237e-04, -4.0772879e-03,\n",
       "        7.7240902e-04, -2.4167877e-03, -1.1275692e-03, -8.2360895e-04,\n",
       "        1.4317539e-03,  3.8500451e-03, -1.5748832e-03,  1.4551851e-03,\n",
       "       -4.0188292e-03,  3.7988760e-03, -2.2271995e-03,  7.6746859e-04,\n",
       "        2.5204935e-03,  9.3139824e-04, -1.2415485e-03,  1.6232274e-03,\n",
       "        1.0698483e-03, -3.0070550e-03, -1.7799556e-03,  4.4001848e-05,\n",
       "        6.3255080e-05, -3.8533583e-03, -9.8826410e-04, -2.1157679e-03,\n",
       "        2.3575821e-03, -5.6829769e-05, -3.1804773e-03,  2.2747214e-03,\n",
       "       -3.1003677e-03, -2.0991061e-03, -7.6263747e-04, -2.4962795e-03,\n",
       "       -4.0762255e-04, -3.6985567e-04, -1.5754134e-03, -6.4150803e-04,\n",
       "       -2.5457437e-03, -4.1685812e-03, -1.9511883e-03, -1.8590546e-03,\n",
       "       -1.7448573e-03, -6.2500924e-04,  1.3027808e-03, -8.7675540e-04,\n",
       "        3.5136661e-03,  1.4592477e-03, -3.8856496e-03,  1.2132961e-03,\n",
       "       -4.8440730e-04,  6.4266467e-04, -1.9680276e-03, -1.1891546e-03,\n",
       "       -3.0678025e-04,  2.5627401e-03,  2.8725206e-03, -2.1907727e-03,\n",
       "        3.9099674e-03,  2.3147935e-05, -1.7299813e-03, -1.9572824e-03,\n",
       "       -1.3761725e-03,  2.0035664e-03, -1.7818508e-03, -3.2945946e-03,\n",
       "        6.9015869e-04,  3.8399380e-03, -3.4945058e-03, -5.7182001e-04],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vecors(['brown','fox'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Creating embedding from dataframe data<h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the sentences to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['token']=train['Phrase'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_w2v=gensim.models.Word2Vec(train['token'], size=500, window=10, min_count=1, iter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector size/embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_w2v.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of words in the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18238"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentiment_w2v.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(list(sentiment_w2v.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('coat', 0.6962974071502686),\n",
       " ('sweaty', 0.6266077756881714),\n",
       " ('shopping', 0.5884391069412231),\n",
       " ('desert', 0.5387845635414124),\n",
       " ('Tyson', 0.5385549068450928),\n",
       " ('lameness', 0.5368521213531494),\n",
       " ('Characterisation', 0.5314506888389587),\n",
       " ('benchmark', 0.511361837387085),\n",
       " ('pastel', 0.5001469254493713),\n",
       " ('porn', 0.4979749321937561)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_w2v.most_similar(['rain'],topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get words similar to 'rain' or 'run'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('coat', 0.5020240545272827),\n",
       " ('shopping', 0.4916538894176483),\n",
       " ('broke', 0.4786890149116516),\n",
       " ('tunnels', 0.4614473581314087),\n",
       " ('anonymous', 0.45422470569610596),\n",
       " ('stand-up-comedy', 0.45180177688598633),\n",
       " ('desert', 0.44589999318122864),\n",
       " ('hack-and-slash', 0.44477319717407227),\n",
       " ('horses', 0.44421201944351196),\n",
       " ('substitutes', 0.4389292001724243)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_w2v.wv.most_similar(positive=['rain','run'],topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get disimilar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'morning'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_w2v.wv.doesnt_match(['raining','bad','morning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get similarity measure between two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15875462"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_w2v.similarity('rain','sun')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load google word2vec pre-trained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim.downloader as api\n",
    "# wv_embedding = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='center'><h1>Conclusion</h1><div />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embedding is a powerful technique of representing text in vector form. This technique ensures that the syntactic and semantic aspect of the text is preserved. By preserving the context of each word in the text we get to infere correctly to the meaning of each word in the discourse of communication. We have looked at Word2Vec which is one of the implementation of Word Embedding. Other Word embedding implementations include glove, and fasttext.<br /><br />\n",
    "One of the challenges of word embedding is the out of vocabulary word problem. The word that was not in the training corpus for embedding cannot be mapped into any vector space. This is a major challenges since the human language keeps evoling and new vocabularies are being invented daily not to mention errors the exisiting vocabularies by users."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
